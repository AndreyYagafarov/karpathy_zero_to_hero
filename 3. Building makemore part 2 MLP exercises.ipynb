{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d30079",
   "metadata": {},
   "source": [
    "- E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2\n",
    "- E02: I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\n",
    "- E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05df94",
   "metadata": {},
   "source": [
    "This notebook starts by organizing the code into a neat summary. As the notebook with the lecture notes followed the lectures, at times it is not tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f3e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5937e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e09179b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8135ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182484, 4]) torch.Size([182484])\n",
      "torch.Size([22869, 4]) torch.Size([22869])\n",
      "torch.Size([22793, 4]) torch.Size([22793])\n"
     ]
    }
   ],
   "source": [
    "# construct training, dev, testing datasets\n",
    "block_size = 3\n",
    "emb_size = 10\n",
    "hid_size = 200\n",
    "\n",
    "def build_dataset(words):\n",
    "#     block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "    \n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb19e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((27,emb_size), generator=g)\n",
    "W1 = torch.randn((block_size * emb_size, hid_size), generator=g) # weights. 6 - 3*2\n",
    "b1 = torch.randn(hid_size, generator=g)\n",
    "W2 = torch.randn((hid_size, 27), generator=g) # 100 inputs, 27 outputs\n",
    "\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "203804c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13897"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a189096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5096ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4dfec03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(200000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,)) # using only the training dataset\n",
    "    \n",
    "    # forward path\n",
    "    emb = C[Xtr[ix]] # using only the training dataset\n",
    "    h = torch.tanh(emb.view(-1, block_size * emb_size) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    \n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) \n",
    "\n",
    "    # backward path\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    lossi.append(loss.log10().item())\n",
    "    stepi.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebf76dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.283578634262085\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9321fe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.190828800201416\n"
     ]
    }
   ],
   "source": [
    "# loss for the dev set\n",
    "emb = C[Xdev] # using the dev set to evaluate the loss\n",
    "h = torch.tanh(emb.view(-1, block_size * emb_size) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ydev) # usind the dev set to evaluate the loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0951df83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.148804187774658\n"
     ]
    }
   ],
   "source": [
    "# loss for the training set\n",
    "emb = C[Xtr] # using the tr set to evaluate the loss\n",
    "# h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "h = torch.tanh(emb.view(-1, block_size * emb_size) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ytr) # usind the tr set to evaluate the loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1726270",
   "metadata": {},
   "source": [
    "- block_size = 3, emb_size = 10, hid_size = 200, steps 200_000, lr = 0.1 if i < 100000 else 0.01. loss_dev = 2.16, loss_train = 2.13\n",
    "- block_size = 3, emb_size = 6, hid_size = 200, steps 200_000, lr = 0.1 if i < 100000 else 0.01. loss_dev = 2.19, loss_train = 2.14\n",
    "- block_size = 3, emb_size = 15, hid_size = 200, steps 200_000, lr = 0.1 if i < 100000 else 0.01. loss_dev = 2.16, loss_train = 2.09\n",
    "- block_size = 3, emb_size = 15, hid_size = 200, steps 300_000, lr = 0.1 if i < 100000 else 0.01. loss_dev = 2.16, loss_train = 2.09\n",
    "- block_size = 4, emb_size = 10, hid_size = 200, steps 200_000, lr = 0.1 if i < 100000 else 0.01. loss_dev = 2.19, loss_train = 2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c585e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "julini.\n",
      "plysey.\n",
      "fure.\n",
      "rhves.\n",
      "kakt.\n",
      "johdisj.\n",
      "joley.\n",
      "ja.\n",
      "aaliani.\n",
      "aadiely.\n",
      "refief.\n",
      "than.\n",
      "janeima.\n",
      "jongyn.\n",
      "frys.\n",
      "layjay.\n",
      "ashan.\n",
      "tref.\n",
      "lion.\n",
      "hwiahuyah.\n"
     ]
    }
   ],
   "source": [
    "# sampling from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 100)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])] # (1, block_size, d)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "        \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e3c7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
